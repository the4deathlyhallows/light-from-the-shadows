Skip to content
Chat history

You said:
what is the tstats command for
ChatGPT said:
In Splunk, the tstats command is a high-performance statistical command used to query indexed fields (especially in accelerated data models or summary indexes) much faster than a regular stats search.
Let’s break it down clearly 👇

🧠 What tstats Does
tstats = “transformed stats”
It lets you:
Run statistical aggregations (count, sum, avg, etc.)
On indexed fields (like index, sourcetype, host, and accelerated CIM fields)
Without fully loading raw events, making it much faster than stats or search.
⚙️ Basic Syntax
| tstats <stats-functions> FROM <datasets> [WHERE <criteria>] [BY <fields>]
💡 Examples
1. Count events by index
| tstats count WHERE index=* BY index
→ Shows total event count per index.
2. Count events by sourcetype
| tstats count WHERE index=security BY sourcetype
→ Counts security index events per sourcetype.
3. Summarize data model fields
If you have a CIM data model accelerated (like Endpoint or Network_Traffic):
| tstats count FROM datamodel=Endpoint.Processes BY "Processes.user", "Processes.process_name"
→ Pulls from the accelerated data model instead of raw logs. This is extremely fast.
4. Filtered example
| tstats count FROM datamodel=Authentication WHERE "Authentication.action"="failure" BY "Authentication.user"
→ Counts failed logins per user using the Authentication data model.
🚀 Why Use tstats Instead of stats
Feature	tstats	stats
Speed	Very fast (metadata-level, no event expansion)	Slower (loads raw events)
Data Source	Indexed fields / accelerated data models	Any field (even unindexed)
Best for	Summarized reports, CIM acceleration, dashboards	Ad-hoc searches, event-level analysis
⚠️ Limitations
Can only use indexed fields or accelerated data model fields.
Cannot extract fields at search time.
You can’t use it directly with fields that don’t exist in the index time metadata.
✅ Quick Summary
tstats is best used when:
You want fast metrics (counts, averages, sums, etc.).
You’re working with large datasets.
You’re using data model acceleration (CIM).
You’re building dashboards or detections that require quick rollups.
Would you like me to show how tstats is typically used in detection engineering (like counting failed logins, process creation anomalies, or network traffic spikes)?
You said:
what about tstats summariesonly=true
ChatGPT said:
Excellent question — this is one that really separates power users and detection engineers from casual Splunk users 👏
Let’s unpack what summariesonly=true in tstats really means.

🧩 summariesonly=true — What It Does
The summariesonly=true argument in a tstats search tells Splunk to only search data model acceleration summaries, and skip raw indexed data completely.
In short:

💡 It forces Splunk to use summary (accelerated) data only — not the original event data.
🔍 Why This Matters
When you accelerate a data model, Splunk builds summary files (optimized pre-aggregated data stored in summary directories).
tstats summariesonly=true tells Splunk:
“Use those summaries, don’t touch the raw index.”
This can make a search 10–100x faster, especially on large datasets.
⚙️ Syntax
| tstats summariesonly=true count FROM datamodel=Authentication BY "Authentication.user"
This:
Pulls only accelerated summary data from the Authentication data model.
Does not look at index= or sourcetype= raw events.
⚡ Example Comparisons
🔸 Without summariesonly (default)
| tstats count FROM datamodel=Authentication WHERE "Authentication.action"="failure"
✅ Uses summaries if they exist,
❌ but may also fall back to raw data if summaries aren’t up-to-date.
🔸 With summariesonly=true
| tstats summariesonly=true count FROM datamodel=Authentication WHERE "Authentication.action"="failure"
✅ Uses only existing accelerated summaries
❌ Ignores any events not yet summarized (e.g., recent events not yet accelerated)
🧠 When to Use It
Use Case	Should You Use summariesonly=true?	Why
Fast dashboards / reports	✅ Yes	Keeps searches lightweight
CIM-based detections using accelerated data models	✅ Yes	Ensures consistent performance
Real-time searches / hunting	⚠️ Maybe	You might miss the most recent events
Investigations (need full fidelity)	❌ No	You want raw logs too
🚨 Pro Tip for Detection Engineers
When you’re writing detections based on CIM models, it’s common to use:
| tstats summariesonly=true count FROM datamodel=Endpoint.Processes BY Processes.user, Processes.process_name
✅ Super fast for dashboards or trend jobs
⚠️ But if you’re testing a new correlation or alert, drop the flag until you verify full event coverage:
| tstats count FROM datamodel=Endpoint.Processes BY Processes.user, Processes.process_name
🧾 Summary Table
Option	Behavior
summariesonly=false	Uses both summaries and raw data (default)
summariesonly=true	Uses only accelerated summaries
(no flag specified)	Defaults to false — Splunk can fall back to raw data
✅ Bottom Line
Use summariesonly=true when:
You’re running scheduled reports, dashboards, or detections on accelerated data models
You prioritize speed and consistency
Avoid it when:
You need real-time or complete coverage (may miss fresh, unaccelerated events)
Would you like me to show an example of how tstats summariesonly=true fits into a real detection search (e.g., process spawning from unusual parent or failed logons across hosts)?
You said:
what about the values command
ChatGPT said:
Excellent — you’re getting into the core of Splunk stats functions, which are the backbone of detection engineering and data summarization.
Let’s go deep but cleanly:

🧩 values() — What It Does
The values() function in Splunk is used with commands like stats, eventstats, or tstats to:
Return all unique values of a field as a multivalue list.
So instead of giving you a count, sum, or average, it gives you a set of distinct values seen in that group.
⚙️ Basic Syntax
| stats values(<field>) BY <grouping-field>
💡 Example 1 — Unique Users per Host
index=security sourcetype=authentication
| stats values(user) AS users BY host
🧠 Meaning:
For each host, list all unique users that logged in.
✅ Output might look like:
host	users
win10-lab	admin, robert, svc_account
srv-prod01	svc_account, domain_admin
💡 Example 2 — Unique Processes per User
| tstats values(Processes.process_name) AS process_list FROM datamodel=Endpoint.Processes BY Processes.user
→ Lists all unique process names per user — very common in detection engineering.
💡 Example 3 — Combine with count for more insight
index=windows EventCode=4688
| stats count values(New_Process_Name) AS unique_processes BY user
✅ Shows how many processes a user ran, plus a list of which ones.
🧠 How It Differs from Other Functions
Function	Purpose	Example Output
values(field)	All unique values	chrome.exe, powershell.exe, cmd.exe
list(field)	All values (including duplicates)	chrome.exe, chrome.exe, cmd.exe
dc(field)	Distinct count	3
count(field)	Count of events with that field	10
⚡ Using values() in tstats
You can absolutely use it inside tstats, like so:
| tstats values(Authentication.user) AS users FROM datamodel=Authentication BY Authentication.src
✅ Fast way to list all users who authenticated from each source IP.
🚀 Real Detection Engineering Examples
🔸 Suspicious Parent Processes
| tstats values(Processes.process_name) AS child_processes 
  FROM datamodel=Endpoint.Processes 
  WHERE Processes.parent_process_name="powershell.exe" 
  BY Processes.dest
→ Shows all unique child processes spawned by PowerShell on each host.
🔸 Failed Logons per User
| tstats values(Authentication.src) AS src_ips 
  count FROM datamodel=Authentication 
  WHERE Authentication.action="failure" 
  BY Authentication.user
→ Lists all unique source IPs that failed to log in for each user.
✅ Summary
Feature	values() Function
Purpose	Collects all unique values of a field
Data Type	Multivalue field (array-like)
Works With	stats, eventstats, tstats, streamstats, etc.
Best For	Summaries, correlation searches, and enrichment tables
Caution	Large fields can inflate memory — use with filters when possible
Would you like me to show you how to use values() together with mvexpand or eval to make detections more actionable (like pivoting from a multivalue list into individual events for correlation)?
You said:
yes
ChatGPT said:
Perfect 🔥 — this is where you move from simple aggregation to powerful correlation logic in Splunk.
Let’s walk through how to use values() together with mvexpand and eval to turn summaries into detection-ready data.

🧩 1. What Happens with values()
As we said, values() creates a multivalue field, meaning one field can hold several values:
user=robert
src_ips=192.168.1.10 192.168.1.25 10.0.0.15
This is great for summarization, but what if you want to act on each IP individually (like match against a threat list)?
That’s where mvexpand comes in.
⚙️ 2. The mvexpand Command
mvexpand turns each value in a multivalue field into its own event (row).
Example:
index=auth
| stats values(src) AS src_ips BY user
| mvexpand src_ips
✅ Output:
user	src_ips
robert	192.168.1.10
robert	192.168.1.25
robert	10.0.0.15
Now you can treat each source IP individually — for enrichment, lookups, or detection logic.
💡 3. Add eval for Data Shaping
You can use eval before or after expansion to make detections smarter.
Example — Normalize field names:

| eval src_ip=src_ips
| fields user src_ip
Or flag suspicious IPs:
| eval is_suspicious=if(match(src_ip,"10\.0\.0\."),"true","false")
🚀 4. Real Detection Engineering Scenarios
🔸 Example 1: Failed Logins From Multiple IPs
| tstats values(Authentication.src) AS src_ips 
        count FROM datamodel=Authentication 
        WHERE Authentication.action="failure" 
        BY Authentication.user
| mvexpand src_ips
| eval src_ip=src_ips
| lookup threat_intel_ips ip AS src_ip OUTPUT severity
| where isnotnull(severity)
✅ What’s happening:
You collect unique failed login IPs per user
Expand them into rows
Compare each against a threat intel feed
🔸 Example 2: Processes Spawned by PowerShell
| tstats values(Processes.process_name) AS child_processes 
  FROM datamodel=Endpoint.Processes 
  WHERE Processes.parent_process_name="powershell.exe" 
  BY Processes.dest
| mvexpand child_processes
| eval process=child_processes
| where match(process, "(cmd\.exe|mshta\.exe|wscript\.exe)")
✅ Flags hosts where PowerShell spawned suspicious children.
🔸 Example 3: Multi-user Correlation (Hunting Style)
| tstats values(Authentication.user) AS users 
  FROM datamodel=Authentication 
  WHERE Authentication.action="success" 
  BY Authentication.src
| mvexpand users
| eval normalized_user=lower(users)
| lookup known_users user AS normalized_user OUTPUT is_admin
| where isnull(is_admin)
✅ Finds successful logins from IPs that authenticated users not recognized as admins.
🧠 Pro Tips for Detection Engineers
Tip	Why It Matters
Filter before expanding	mvexpand multiplies rows — filter early to save memory
Use values() → mvexpand → lookup pattern	Standard correlation flow
Use mvcount(field) before expanding	Helps see how “wide” your multivalue fields are
Use `	makemv delim="," fieldname`
🧾 Summary
Command	Purpose
values(field)	Collects all unique values of a field into one row
mvexpand field	Splits multivalue field into separate events
eval	Used for renaming, normalizing, or flagging each value after expansion
lookup	Common next step — correlate each value to intel or baselines
Would you like me to build you a template detection search that chains all of these together — tstats + values() + mvexpand + lookup — in a real-world detection use case (e.g., “Suspicious Login Sources” or “PowerShell Child Process Activity”)?
You said:
yes
ChatGPT said:
🔥 Perfect — let’s build you a detection engineer–level Splunk template that combines all the power tools you just asked about:
tstats → values() → mvexpand → eval → lookup

We’ll do this using two real-world detection use cases so you can reuse the pattern across multiple scenarios.

⚔️ Detection Template 1: Suspicious Login Sources (Authentication Data Model)
Goal: Detect users logging in from multiple or unusual IPs, possibly linked to threat intelligence.
🧠 Search Breakdown
| tstats summariesonly=true count 
        values(Authentication.src) AS src_ips 
  FROM datamodel=Authentication 
  WHERE Authentication.action="success" 
  BY Authentication.user, Authentication.app

| mvexpand src_ips

| eval src_ip = src_ips
| fields Authentication.user Authentication.app src_ip count

| lookup threat_intel_ips ip AS src_ip OUTPUT severity, category

| eval severity=coalesce(severity,"none")
| where severity!="none"

| table Authentication.user Authentication.app src_ip severity category count
| rename Authentication.user AS user Authentication.app AS app
💡 What This Does
tstats summariesonly=true → fast pull from CIM Authentication model
values(Authentication.src) → collect all unique login source IPs per user
mvexpand → split into one row per IP
lookup → compare each IP against a threat intel lookup
where → return only matches (bad IPs)
rename/table → clean up for readability
✅ Example Output
user	app	src_ip	severity	category	count
jdoe	owa	203.0.113.45	high	C2	15
asmith	vpn	185.44.55.22	medium	TOR Exit	8
⚙️ Detection Template 2: Suspicious PowerShell Child Processes (Endpoint Data Model)
Goal: Detect when PowerShell spawns unusual child processes, like cmd.exe, rundll32.exe, or mshta.exe.
🧠 Search Breakdown
| tstats summariesonly=true count 
        values(Processes.process_name) AS child_procs 
  FROM datamodel=Endpoint.Processes 
  WHERE Processes.parent_process_name="powershell.exe" 
  BY Processes.dest, Processes.user

| mvexpand child_procs
| eval child_process = lower(child_procs)

| where match(child_process, "(cmd\.exe|mshta\.exe|rundll32\.exe|wscript\.exe|regsvr32\.exe)")

| lookup baseline_processes process_name AS child_process OUTPUT known_good

| where isnull(known_good)

| table Processes.dest Processes.user child_process count
| rename Processes.dest AS host Processes.user AS user
💡 What This Does
Uses accelerated Endpoint.Processes data model for speed
Collects unique child processes launched by PowerShell
Expands them into individual rows
Filters for known suspicious binaries
Cross-references a baseline lookup (baseline_processes.csv)
Filters out benign entries
✅ Example Output
host	user	child_process	count
win10-lab	admin	rundll32.exe	3
srv01	svc_backup	mshta.exe	2
🧩 Reusable Detection Pattern
You can reuse this pattern across most CIM data models:
| tstats summariesonly=true values(<Model.Field>) AS <alias> count 
  FROM datamodel=<Model>.<Dataset>
  WHERE <filter>
  BY <grouping-fields>
| mvexpand <alias>
| eval <normalized_field>=lower(<alias>)
| lookup <lookup_table> <lookup_field> AS <normalized_field> OUTPUT <lookup_output_fields>
| where <filter_condition>
| table <fields_you_want>
🚀 Bonus Tip:
When tuning detections:
Add | eval mv_count=mvcount(src_ips) to see spread (e.g., how many IPs per user)
Use | where mv_count>5 to catch abnormally distributed access patterns
Combine values() + dc() to both list and count unique values
Would you like me to show you how to convert one of these detections into a correlation search (with adaptive response and risk scoring) inside Enterprise Security next? That’s the step that turns this logic into an automated detection.



ChatGPT can make mistakes. Check important info.
